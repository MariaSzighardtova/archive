%pip install -r requirements.txt;
##  Importing libraries
import numpy as np 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
## Load the data
data = pd.read_csv("data.csv")
data.head()
data.shape
## Data Preprocessing
data.info()
data.drop("Unnamed: 32", axis=1, inplace = True)
data.drop("id", axis=1, inplace = True)
data.describe()
## Exploratory Data Analysis (EDA)
data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})
data.head()

columns_to_plot = data.columns.drop("diagnosis")

plt.figure(figsize=(20, 15))
plotnumber = 1
num_columns = 5  

for column in columns_to_plot:
    plt.subplot(len(columns_to_plot) // num_columns + 1, num_columns, plotnumber)
    sns.kdeplot(data[data['diagnosis'] == 1][column], label='Malígne', fill=True)
    sns.kdeplot(data[data['diagnosis'] == 0][column], label='Benígne', fill=True)
    plt.title(f'{column}')
    plt.legend()
    plotnumber += 1

plt.tight_layout() 
plt.show()

Graf zobrazuje hustotu rozdelenia hodnôt pre malígne nádory (M) - modrá línia a pre benígne nádory (B) - oranžovaná línia

* vo väčšine je zrejmý rozdiel medzi M a B nádormi:
** M sú väčšie (radius_mean, perimeter_mean, area_mean)
** M majú nepravidelné tvary (compactness_mean, concavity_mean, concave points_mean)
** M majú zložitý povrch (vyšší fractal_dimension_mean)
plt.figure(figsize = (20, 12))

corr = data.corr()
mask = np.triu(np.ones_like(corr, dtype = bool))

sns.heatmap(corr, mask = mask, linewidths = 1, annot = True, fmt = ".2f")
plt.show()
Heatmapa korealčnej matice identifikuje vzťahy medzi atribútami.

Našou premennou je "diagnosis" a hľadáme najvyššie korelácie medzi premennou a atribútami.
corr_matrix = data.corr().abs()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

tri_df = corr_matrix.mask(mask)
to_drop = [x for x in tri_df.columns if any(tri_df[x] > 0.92)]

data_reduced = data.drop(to_drop, axis=1)
print(f"Zredukovaný dataset má {data_reduced.shape[1]} stĺpcov.")
print(f"Odstránené stĺpce: {to_drop}")
* Korelačná matica vypočíta korelácie medzi všetkými atribútmi a použije absolútne hodnoty, aby sa zaistilo, že berieme do úvahy aj záporné korelácie.
* Maska: np.triu vytvára masku pre horný trojuholník korelačnej matice, takže neberieme do úvahy duplicity (keďže korelačná matica je symetrická).
* Odstránenie stĺpcov: Stĺpce, kde je hodnota korelácie vyššia ako 0.92, sú označené na odstránenie. Tým sa zníži redundantnosť dát.
* Výpis počtu stĺpcov: Kód vypíše počet stĺpcov po znížení počtu atribútov a tiež zobrazí, ktoré atribúty boli odstránené.

* po redukcii počtu atribútov, pokračujeme s analýzou a modelavním využitím:
    logistickej regresie
    Random forest
    K-Nearest Neighbors...
## Modelovanie
X = data.drop('diagnosis', axis=1)
y = data['diagnosis']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
* Rozdelili sme dáta na tréningovú a testovaciu množinu:
* Pomocou StandardScaler normalizujeme hodnoty tak, aby mali priemer 0 a rozptyl 1, čo zabezpečí, že všetky atribúty budú na rovnakej škále a model bude efektívnejší.
* fit_transform aplikuje škálovanie na tréningovú množinu, pričom transform aplikuje rovnaké škálovanie na testovaciu množinu bez opätovného "učenia".
## Lineárna regresia
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred = log_reg.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print(accuracy_score(y_train, log_reg.predict(X_train)))

log_reg_acc = accuracy_score(y_test, log_reg.predict(X_test))
print(log_reg_acc)
Výstup ukazuje, že model Lineárnej regresie dosahuje veľmi dobré výsledky s vysokou presnosťou, odvolaním a F1-skóre pre obe triedy, a celková presnosť je 98%.
from sklearn.model_selection import learning_curve

def plot_learning_curve(estimator, X, y):
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5))

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    plt.figure()
    plt.plot(train_sizes, train_scores_mean, label="Tréningové skóre")
    plt.plot(train_sizes, test_scores_mean, label="Krížová validácia")
    plt.xlabel("Veľkosť tréningových dát")
    plt.ylabel("Presnosť")
    plt.title("Krivka učenia")
    plt.legend(loc="best")
    plt.grid()
    plt.show()

plot_learning_curve(log_reg, X_train, y_train)

Na krivke učenia modelu vidíme zemnu presnosti modelu v závislosti od veľkosti trénovacích dát. 
*  Tréningové skóre (modrá krivka): na začiatku tréningové skóre je vysoké, znamená, že model na tréningových dátach dokáže dosiahnúť takmer dokonalú presnosť. Zvyšovaním tréningových dát, skóre mierne klesá. Pokles je bežný a ukazuje, že model sa učí viac generalizovať a prestáva byť príliš optimalizovaný len na tréningové dáta.

* Krížová validácia (oranžová krivka): na začiatku má nižšiu presnosť, pretože model nemá dostatočné množstvo dát. Pridaním dát, presnosť modelu narastá. Krivka sa postupne približuje k tréningovej krivke, čo znamená, že model má dobrú schopnosť generalizovať a nie je preučený.
## k-Nearest Neighbors (KNN)
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

print(accuracy_score(y_train, knn.predict(X_train)))

knn_acc = accuracy_score(y_test, knn.predict(X_test))
print(knn_acc)

import warnings
warnings.filterwarnings("ignore", message=".*physical cores.*")
* Tréningová presnosť: 0.9723 (97.23%) – model dosahuje veľmi dobrú presnosť na trénovacích dátach.
* Testovacia presnosť: 0.9590 (95.90%) – model tiež dosahuje vysokú presnosť na neznámych dátach.
def plot_learning_curve_knn(estimator, X, y):
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5))

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)

    plt.figure()
    plt.plot(train_sizes, train_scores_mean, label="Tréningové skóre")
    plt.plot(train_sizes, test_scores_mean, label="Krížová validácia")
    plt.xlabel("Veľkosť tréningových dát")
    plt.ylabel("Presnosť")
    plt.title("Krivka učenia - KNN")
    plt.legend(loc="best")
    plt.grid()
    plt.show()

plot_learning_curve_knn(knn, X_train, y_train)
## ANN
# Porovnanie modelov neurónových sietí
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Input
## MODEL č.1

* Jednoduchá neurónová sieť s jednou skrytou vrstvou a dropoutom.
* Optimalizátor Adam s adaptívnym učením.
* Normalizácia dát pomocou StandardScaler.
* Krížová validácia (KFold Cross-Validation), ktorá zabezpečila robustnejšie hodnotenie výkonnosti modelu na rôznych podmnožinách dát.
from keras.optimizers import Adam
from sklearn.model_selection import KFold

def create_model():
    model = Sequential()
    model.add(Input(shape=(22,)))
    model.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))
    model.add(Dropout(rate=0.1))
    model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

X = data_reduced.iloc[:, 1:]  
y = data_reduced.iloc[:, 0]    

scaler = StandardScaler()
X = scaler.fit_transform(X)

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
losses = []

for train_index, val_index in kfold.split(X, y):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    classifier = create_model()

    classifier.fit(X_train, y_train, batch_size=100, epochs=150, verbose=0)

    score = classifier.evaluate(X_val, y_val, verbose=0)
    accuracies.append(score[1])
    losses.append(score[0])

mean_accuracy1 = np.mean(accuracies)
mean_loss1 = np.mean(losses)

print(f'Priemerná presnosť: {mean_accuracy1}')
print(f'Priemerná strata: {mean_loss1}')
## MODEL č.2

# úprava modelu:
* Zložitejšia architektúra s dvoma vrstvami, kde prvá vrstva má 32 neurónov a druhá vrstva 16 neurónov.
* Použitie Dropoutu s hodnotou 0.2, aby sa znížilo pretrénovanie.
* Optimalizátor Adam s adaptívnym učením.
* Krížová validácia (KFold Cross-Validation) s 5 foldmi, aby sme zabezpečili robustné hodnotenie výkonnosti modelu na rôznych podmnožinách dát.
X = data_reduced.iloc[:, 1:]  
y = data_reduced.iloc[:, 0]   

scaler = StandardScaler()
X = scaler.fit_transform(X)

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
losses = []

for train_index, val_index in kfold.split(X, y):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    classifier = Sequential()
    classifier.add(Input(shape=(22,)))  
    classifier.add(Dense(units=32, kernel_initializer='uniform', activation='relu'))
    classifier.add(Dropout(rate=0.2))  
    classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))
    classifier.add(Dropout(rate=0.2))
    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))  

    classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

    classifier.fit(X_train, y_train, batch_size=100, epochs=150, verbose=0)

    score = classifier.evaluate(X_val, y_val, verbose=0)

    accuracies.append(score[1])
    losses.append(score[0])

mean_accuracy2 = np.mean(accuracies)
mean_loss2 = np.mean(losses)

print(f'Priemerná presnosť: {mean_accuracy2}')
print(f'Priemerná strata: {mean_loss2}')

Zmeny spolu viedli k zvýšeniu presnosti modelu, keďže priemerná presnosť sa zvýšila na 93 % a model robí menšie chyby (nižšia strata). Viac vrstiev, dropout a krížová validácia pomohli modelu lepšie sa prispôsobiť dátam a znížiť pretrénovanie.
## MODEL č.3

* # úprava modelu:
* Zložitejšia architektúra modelu: Dve skryté vrstvy s väčším počtom neurónov (64 a 32), ktoré lepšie zachytávajú komplexné vzory v dátach.
* Použitie PCA (Principal Component Analysis):PCA na zníženie dimenzionality na 10 hlavných komponentov, čo zlepšuje efektivitu a znižuje šum.
* Dropout s vyššou hodnotou (0.3): Použitie Dropoutu na oboch skrytých vrstvách pomáha znížiť riziko pretrénovania.
* Early Stopping: Early Stopping zabezpečuje, že tréning sa zastaví, keď sa model prestane zlepšovať, čím sa predchádza zbytočnému trénovaniu a riziku pretrénovania.
* Krížová validácia (KFold Cross-Validation):Krížová validácia so shuffle poskytuje stabilné vyhodnotenie modelu na rôznych podmnožinách dát, čím sa zabezpečí lepšia generalizácia.
from keras.callbacks import EarlyStopping
from sklearn.decomposition import PCA

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  
y = data_reduced.iloc[:, 0]         

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

accuracies = []
losses = []

for train_index, val_index in kfold.split(X_scaled, y):
    X_train, X_val = X_scaled[train_index], X_scaled[val_index]
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    pca = PCA(n_components=10)
    X_train = pca.fit_transform(X_train)
    X_val = pca.transform(X_val)

    classifier = Sequential()
    classifier.add(Input(shape=(10,))) 
    classifier.add(Dense(units=64, kernel_initializer='uniform', activation='relu'))
    classifier.add(Dropout(rate=0.3))
    classifier.add(Dense(units=32, kernel_initializer='uniform', activation='relu'))
    classifier.add(Dropout(rate=0.3))
    classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

    classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    classifier.fit(X_train, y_train, batch_size=100, epochs=150, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)

    score = classifier.evaluate(X_val, y_val, verbose=0)
    accuracies.append(score[1]) 
    losses.append(score[0])      

mean_accuracy3 = np.mean(accuracies)
mean_loss3 = np.mean(losses)

print(f'Priemerná presnosť pre model č. 3: {mean_accuracy3}')
print(f'Priemerná strata pre model č. 3: {mean_loss3}')

# Zhrnutie:

* Model č. 1: Je najjednoduchší a dosiahol najnižšiu presnosť (90.51 %) a najvyššiu stratu (0.4170). Má iba jednu skrytú vrstvu a menší dropout. Zlepšenie by mohlo zahŕňať pridaním ďalšej vrstvy a zvýšením Dropoutu.

* Model č. 2: Má dve skryté vrstvy, väčší počet neurónov a vyšší Dropout (0.2), čo vedie k lepším výsledkom ako model č. 1 (93.85 % presnosť). Model má však stále potenciál na zlepšenie, najmä pridaním techník ako Early Stopping a PCA.

* Model č. 3: Tento model dosiahol najlepšiu presnosť (96.31 %) a najnižšiu stratu (0.1526) vďaka pridaniu PCA a Early Stopping. Kombinácia väčšieho počtu neurónov, vyššieho Dropoutu, redukcie dimenzionality a zastavenia tréningu pri poklese zlepšenia priniesla výrazné zlepšenie.

* Model č. 3 je preto najefektívnejší z hľadiska generalizácie, schopnosti zachytiť vzory v dátach a zníženia pretrénovania.

models = ['Model č. 1', 'Model č. 2', 'Model č. 3']

accuracy = [mean_accuracy1, mean_accuracy2, mean_accuracy3]
loss = [mean_loss1, mean_loss2, mean_loss3]

plt.figure(figsize=(8, 4))

plt.subplot(1, 2, 1)
plt.bar(models, accuracy, color='lightblue')
plt.ylim(0.8, 1.0)
plt.title('Porovnanie presnosti modelov')
plt.xlabel('Modely')
plt.ylabel('Priemerná presnosť')

plt.subplot(1, 2, 2)
plt.bar(models, loss, color='salmon')
plt.ylim(0, 0.5)
plt.title('Porovnanie straty modelov')
plt.xlabel('Modely')
plt.ylabel('Priemerná strata')

plt.tight_layout()
plt.show()
